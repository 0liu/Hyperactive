# Optimization Techniques

Hyperactive offers a wide variety of basic, meta-heuristic and sequential model-based optimization techniques for machine learning model selection and hyperparameter tuning. This readme provides an overview[*](https://github.com/SimonBlanke/Hyperactive/tree/master/hyperactive/optimizers#disclaimer) and brief explainations of those techniques and proposes a possible field of application.



---

<p align="center">
  <a href="https://github.com/SimonBlanke/Hyperactive/tree/master/hyperactive/optimizers#local-search">Local Search</a> |
  <a href="https://github.com/SimonBlanke/Hyperactive/tree/master/hyperactive/optimizers#random-methods">Random Methods</a> |
  <a href="https://github.com/SimonBlanke/Hyperactive/tree/master/hyperactive/optimizers#markov-chain-monte-carlo">Markov Chain Monte Carlo</a> |
  <a href="https://github.com/SimonBlanke/Hyperactive/tree/master/hyperactive/optimizers#population-methods">Population Methods</a>
</p>

---


## Local Search:

#### Hill Climbing

#### Stochastic Hill Climbing

#### Tabu Search

## Random Methods:

#### Random Search

#### Random Restart Hill Climbing

#### Random Annealing


## Markov Chain Monte Carlo:

#### Simulated Annealing

#### Stochastic Tunneling

#### Parallel Tempering

## Population Methods:

#### Particle Swarm Optimizer

#### Evolution Strategy

## Sequential Methods:

#### Bayesian Optimization




##### Disclaimer:
The classification into the categories above is not necessarly scientificly accurate, but aims to provide an idea of the functionality of the methods.

